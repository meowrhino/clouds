name: Extraer ainfo desde Neocities

on:
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # cada día a las 3am UTC

permissions:
  contents: write

jobs:
  scrap:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Instala dependencias
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Ejecuta scraper Python
        run: |
          python - <<'PY'
          import json, requests
          from bs4 import BeautifulSoup, Comment

          def main():
              url = "https://meowrhino.neocities.org/"
              r = requests.get(
                  url,
                  timeout=30,
                  headers={"User-Agent": "Mozilla/5.0 (GitHub Actions scraper)"}
              )
              r.raise_for_status()

              soup = BeautifulSoup(r.content, "html.parser")
              cont = soup.find(id="proyectes")
              if cont is None:
                  raise SystemExit("No se encontró #proyectes en la página")

              data = {}                 # {"main quests": [ {name,links}, ... ], ...}
              current_cat = None
              current_name = None
              current_links = []

              def flush():
                  nonlocal current_name, current_links
                  if current_name and current_links:
                      cat = current_cat or "uncategorized"
                      data.setdefault(cat, []).append({
                          "name": current_name,
                          "links": current_links
                      })
                  current_name = None
                  current_links = []

              for el in cont.children:
                  if isinstance(el, Comment):
                      flush()
                      current_cat = el.strip()
                      if current_cat:
                          data.setdefault(current_cat, [])
                      continue

                  tag = getattr(el, "name", None)
                  if tag == "ainfo":
                      flush()
                      current_name = el.get_text(strip=True)
                      current_links = []
                  elif tag == "a" and current_name:
                      href = el.get("href")
                      if href:
                          current_links.append(href)
                  elif tag == "br":
                      flush()

              flush()  # por si el último bloque no cerró con <br>

              with open("proyectos.json", "w", encoding="utf-8") as f:
                  json.dump(data, f, indent=2, ensure_ascii=False)

              print("Categorías:", ", ".join(f"{k}({len(v)})" for k, v in data.items()))

          if __name__ == "__main__":
              main()
          PY

      - name: Commit y push si hay cambios
        env:
          BRANCH: ${{ github.ref_name }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add proyectos.json || true
          if git diff --cached --quiet; then
            echo "Sin cambios"
            exit 0
          fi
          git pull --rebase --autostash origin "$BRANCH" || true
          git commit -m "actualiza proyectos por categorías desde Neocities [skip ci]"
          git push origin HEAD:"$BRANCH"
